{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c1c2d9e",
   "metadata": {},
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6dbb955",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics\n",
    "!pip install bytetracker\n",
    "!pip install supervision==0.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36cf3457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 모델 초기화\n",
    "model = YOLO('yolov8m.pt')\n",
    "\n",
    "# 데이터셋 경로 지정\n",
    "data_path = '/home/piai/posco/ai/project/modeling/dataset/data.yaml'  # YAML 파일경로지정하기(pc에있는거)\n",
    "\n",
    "# 모델 학습\n",
    "model.train(data=data_path, epochs=35, imgsz=640, batch=16) # 에폭수 이런건 나중에 내가조절할 것"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49a1c2",
   "metadata": {},
   "source": [
    "## zone counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c30cced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 (no detections), 288.6ms\n",
      "Speed: 3.0ms preprocess, 288.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 311.3ms\n",
      "Speed: 1.2ms preprocess, 311.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 248.4ms\n",
      "Speed: 1.1ms preprocess, 248.4ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 254.3ms\n",
      "Speed: 1.0ms preprocess, 254.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 259.4ms\n",
      "Speed: 2.2ms preprocess, 259.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 264.9ms\n",
      "Speed: 1.1ms preprocess, 264.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 243.5ms\n",
      "Speed: 2.1ms preprocess, 243.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 286.0ms\n",
      "Speed: 3.1ms preprocess, 286.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 253.5ms\n",
      "Speed: 1.4ms preprocess, 253.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 265.6ms\n",
      "Speed: 2.2ms preprocess, 265.6ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 256.5ms\n",
      "Speed: 1.0ms preprocess, 256.5ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 274.0ms\n",
      "Speed: 2.5ms preprocess, 274.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 266.7ms\n",
      "Speed: 2.1ms preprocess, 266.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 304.9ms\n",
      "Speed: 1.0ms preprocess, 304.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 271.3ms\n",
      "Speed: 4.0ms preprocess, 271.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 239.7ms\n",
      "Speed: 2.2ms preprocess, 239.7ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 248.8ms\n",
      "Speed: 2.1ms preprocess, 248.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 253.3ms\n",
      "Speed: 2.1ms preprocess, 253.3ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 244.4ms\n",
      "Speed: 2.3ms preprocess, 244.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 233.6ms\n",
      "Speed: 3.0ms preprocess, 233.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 252.3ms\n",
      "Speed: 2.4ms preprocess, 252.3ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 253.9ms\n",
      "Speed: 1.0ms preprocess, 253.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 255.7ms\n",
      "Speed: 1.0ms preprocess, 255.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 257.8ms\n",
      "Speed: 2.1ms preprocess, 257.8ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 229.4ms\n",
      "Speed: 1.4ms preprocess, 229.4ms inference, 0.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 (no detections), 223.9ms\n",
      "Speed: 1.1ms preprocess, 223.9ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# train63,,이런거 숫자 바꿔줄것, path경로는 주피터에 있는걸로 사용\n",
    "model = YOLO('best.pt')\n",
    "\n",
    "import cv2\n",
    "\n",
    "# 인덱스가 1갠데 이상한거 써서 오류났던거, 해결완료\n",
    "car_class_index = 0\n",
    "\n",
    "def filter_boxes(boxes, names):\n",
    "    filtered_boxes = []\n",
    "    for box_data in boxes.data:\n",
    "        x1, y1, x2, y2, conf, cls_idx = box_data\n",
    "        if cls_idx.item() == car_class_index:  # Filter only for cars\n",
    "            cls_name = names[int(cls_idx.item())]\n",
    "            filtered_boxes.append((cls_name, conf.item(), [x1.item(), y1.item(), x2.item(), y2.item()]))\n",
    "    return filtered_boxes\n",
    "\n",
    "def count_zone(bbox, frame_width, frame_height):\n",
    "    zones = [\n",
    "        (0, 0, frame_width//3, frame_height//3),\n",
    "        (frame_width//3, 0, 2*frame_width//3, frame_height//3),\n",
    "        (2*frame_width//3, 0, frame_width, frame_height//3),\n",
    "        (0, frame_height//3, frame_width//3, 2*frame_height//3),\n",
    "        (frame_width//3, frame_height//3, 2*frame_width//3, 2*frame_height//3),\n",
    "        (2*frame_width//3, frame_height//3, frame_width, 2*frame_height//3),\n",
    "        (0, 2*frame_height//3, frame_width//3, frame_height),\n",
    "        (frame_width//3, 2*frame_height//3, 2*frame_width//3, frame_height),\n",
    "        (2*frame_width//3, 2*frame_height//3, frame_width, frame_height)\n",
    "    ]\n",
    "    zone_counts = [0] * len(zones)\n",
    "    for i, zone in enumerate(zones):\n",
    "        if bbox[0] < zone[2] and bbox[2] > zone[0] and bbox[1] < zone[3] and bbox[3] > zone[1]:\n",
    "            zone_counts[i] += 1\n",
    "    return zone_counts\n",
    "\n",
    "def draw_zones(annotated_frame, frame_width, frame_height):\n",
    "    # Draw lines for each zone with a more visible color and thicker lines\n",
    "    # Vertical lines\n",
    "    cv2.line(annotated_frame, (frame_width//3, 0), (frame_width//3, frame_height), (0, 255, 255), 4)\n",
    "    cv2.line(annotated_frame, (2*frame_width//3, 0), (2*frame_width//3, frame_height), (0, 255, 255), 4)\n",
    "    # Horizontal lines\n",
    "    cv2.line(annotated_frame, (0, frame_height//3), (frame_width, frame_height//3), (0, 255, 255), 4)\n",
    "    cv2.line(annotated_frame, (0, 2*frame_height//3), (frame_width, 2*frame_height//3), (0, 255, 255), 4)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "\n",
    "    if success:\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = model(frame_rgb)\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            filtered_results = filter_boxes(results[0].boxes, results[0].names)\n",
    "            \n",
    "            total_zone_counts = [0] * 9\n",
    "            \n",
    "            annotated_frame = frame.copy()\n",
    "            for name, conf, bbox in filtered_results:\n",
    "                bbox = [int(coord) for coord in bbox]\n",
    "                \n",
    "                zone_counts = count_zone(bbox, frame.shape[1], frame.shape[0])\n",
    "                total_zone_counts = [sum(x) for x in zip(total_zone_counts, zone_counts)]\n",
    "                \n",
    "                cv2.rectangle(annotated_frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 255, 0), 2)\n",
    "                cv2.putText(annotated_frame, f'{name} {conf:.2f}', (bbox[0], bbox[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "            \n",
    "            # Draw zones on the annotated frame\n",
    "            draw_zones(annotated_frame, frame.shape[1], frame.shape[0])\n",
    "            \n",
    "            for i, count in enumerate(total_zone_counts):\n",
    "                cv2.putText(annotated_frame, f'Zone {i+1}: {count}', (10, 20*(i+1)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "\n",
    "            cv2.imshow(\"Filtered YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb903d",
   "metadata": {},
   "source": [
    "## in/out counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d145c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line Counter Initiated.\n",
      "Line Counter Initiated.\n",
      "Line Counter Initiated.\n",
      "Line Counter Initiated.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import object_counter\n",
    "\n",
    "# YOLO 모델 로드\n",
    "model = YOLO(\"best.pt\")\n",
    "\n",
    "# 비디오 파일 열기\n",
    "video_path = \"final_1.mov\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "\n",
    "\n",
    "# 라벨 위치 정의\n",
    "labels_positions = [\n",
    "    ((600, 50), \"Line 1\"),  # Line 1 레이블 위치\n",
    "    ((600, 550), \"Line 2\"),  # Line 2 레이블 위치\n",
    "    ((290, 300), \"Line 3\"),  # Line 3 레이블 위치\n",
    "    ((900, 300), \"Line 4\")   # Line 4 레이블 위치\n",
    "]\n",
    "\n",
    "# 중앙 정사각형 좌표 정의\n",
    "center_x, center_y = 600, 400  # 1920x1080 해상도의 중앙\n",
    "half_size = 200  # 정사각형 반 변의 길이 (총 200 픽셀)\n",
    "\n",
    "# 카운팅할 라인 포인트 (정사각형 모양)\n",
    "line_points = [\n",
    "    [(center_x - half_size, center_y - half_size), (center_x + half_size, center_y - half_size)],  # 첫 번째 카운팅 라인 (상단 가로)\n",
    "    [(center_x - half_size, center_y + half_size), (center_x + half_size, center_y + half_size)],  # 두 번째 카운팅 라인 (하단 가로)\n",
    "    [(center_x - half_size, center_y - half_size), (center_x - half_size, center_y + half_size)],  # 세 번째 카운팅 라인 (좌측 세로)\n",
    "    [(center_x + half_size, center_y - half_size), (center_x + half_size, center_y + half_size)]   # 네 번째 카운팅 라인 (우측 세로)\n",
    "]\n",
    "\n",
    "classes_to_count = [0]  # 카운팅할 클래스 인덱스 (예: 차량)\n",
    "\n",
    "# 라벨 위치 정의\n",
    "labels_positions = [\n",
    "    ((570, 170), \"Line 1\"),  # Line 1 레이블 위치\n",
    "    ((570,630), \"Line 2\"),  # Line 2 레이블 위치\n",
    "    ((290, 312), \"Line 3\"),  # Line 3 레이블 위치\n",
    "    ((820, 312), \"Line 4\")   # Line 4 레이블 위치\n",
    "]\n",
    "\n",
    "# ObjectCounter 객체 초기화\n",
    "counters = [object_counter.ObjectCounter() for _ in range(4)]\n",
    "for counter, pts in zip(counters, line_points):\n",
    "    counter.set_args(view_img=False,\n",
    "                     reg_pts=pts,\n",
    "                     classes_names=model.names,\n",
    "                     draw_tracks=False,\n",
    "                     line_thickness=2)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"웹캠에서 프레임을 가져오는 데 실패했습니다.\")\n",
    "        break\n",
    "    \n",
    "    # 객체 추적 및 특정 클래스 카운트\n",
    "    tracks = model.track(im0, persist=True, show=False, classes=classes_to_count)\n",
    "\n",
    "    # 객체 카운팅 프로세스 시작 및 결과 출력\n",
    "    print(\"각 라인별 합계:\")\n",
    "    for i, (counter, (pos, label)) in enumerate(zip(counters, labels_positions)):\n",
    "        im0 = counter.start_counting(im0, tracks)\n",
    "        if i in [0, 2]:  # Line 1과 Line 3은 out만\n",
    "            total_counts = counter.out_counts\n",
    "        else:            # Line 2와 Line 4는 in만\n",
    "            total_counts = counter.in_counts\n",
    "        \n",
    "        # 레이블 위치 조정 및 텍스트 출력\n",
    "        pos = (int(pos[0]), int(pos[1]))\n",
    "        cv2.putText(im0, f\"Count: {total_counts}\", pos, cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "        print(f\"{label}: {total_counts}\")\n",
    "\n",
    "    cv2.imshow(\"Object Counting\", im0)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# 정리\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9da81ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
